<html>

<head>
    <meta charset="UTF-8">
    <title>Fuzzy-Anki</title>

    <link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="css/anki.css">


    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js" charset="utf-8"></script>
    <script src="js/apkg.js" charset="utf-8"></script>
    <script src="js/bootstrap.min.js" charset="utf-8"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://kit.fontawesome.com/49315bed71.js" crossorigin="anonymous"></script>
</head>

<body>
    <div class="container-fluid">
        <div class="row quizz-header">
            <div class="col-md-4">Quizz: <span id="quizz-title"></span></div>
            <div class="col-md-3" class="card-grade">
                Grade: <span id="card-grade-value">0</span>%
            </div>
            <div class="offset-md-4 col-md-1" id="quizz-info">
                <button class="btn btn-outline-secondary" data-toggle="modal" data-target="#help-modal"><i
                        class="far fa-question-circle"></i></button>
            </div>
        </div>
        <div class="row ankiapp" id="ankiapp">
            <div class="col-md-2 offset-md-1 card-button" id="previous-button">
                <i class="fas fa-arrow-left"></i>
                Previous card
            </div>
            <div class="col-md-6" id="anki">
                <div id="card">
                    <div id="card-front"></div>
                    <p id="card-helper">Click to reveal answer</p>
                    <div id="card-global-back">
                        <div id="card-back"></div>
                        <div id="card-details1"></div>
                        <div id="card-details2"></div>
                        <div id="card-image"></div>
                    </div>
                </div>
            </div>
            <div class="col-md-2 offset-md-1 card-button" id="after-button" style="visibility: hidden">
                Next card
                <i class="fas fa-arrow-right"></i></div>
        </div>
        <div class="row">
            <button id="btn-fail" class="offset-md-5 col-md-1 btn btn-outline-danger"
                style="visibility: hidden">Fail</button>
            <button id="btn-success" class="col-md-1 btn btn-outline-success"
                style="visibility: hidden">Success</button>
        </div>
        <div id="card-counter"></div>
    </div>

    <!-- Modal -->
    <div class="modal fade" id="help-modal" tabindex="-1" role="dialog" aria-labelledby="exampleModalLabel"
        aria-hidden="true">
        <div class="modal-dialog" role="document">
            <div class="modal-content">
                <div class="modal-header">
                    <h5 class="modal-title" id="exampleModalLabel">Information</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                    </button>
                </div>
                <div class="modal-body">
                    <p>1. Read the question and answer in your head.</p>
                    <p>2. Click on the card (or press &darr; on your keyboard) to reveal the answer.</p>
                    <p>3. Grade yourself honestly: fail (q) or success (w). No one is watching you so no need to cheat!
                    </p>
                    <p>4. Go on the next card &rarr;</p>

                    <p>Reset cards <button id="card-reset" class="btn btn-outline-secondary"><i
                                class="fas fa-redo"></i></button></p>
                    <p>You can also download offline version of these cards with <a
                            href="https://apps.ankiweb.net/">Anki</a>.
                        <button class="btn btn-outline-secondary"><i class=" fas fa-download"></i></button>
                    </p>
                </div>
                <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                </div>
            </div>
        </div>
    </div>
</body>

<script>
    var deckNotes = [{ "Front": "<b>Bernoulli </b>distribution", "Back": "<b>Discrete </b>probability distribution with random values <b>0 and 1</b>.<div><img src=\"paste-cd5dc372df153c076a0feacf1c185b75082d35c6.jpg\"><br></div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "Density estimation", "Back": "<div> <div> <div> <div> <div>Given a <b>finite set</b> \\(x_1, . . . , x_N\\) of observations, <b>find distribution</b> \\(p(x)\\) of \\(x\\).&nbsp;</div> </div> </div> </div></div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "With <b>bernoulli</b>, what does represent \\(\\mu\\) here:<div><br></div><div>\\(\\operatorname{Bern}(x | \\mu)=\\mu^{x}(1-\\mu)^{1-x}\\)</div>", "Back": "The <b>prior </b>on the random event.<div><br></div><div><i>With a coin flip, \\(\\mu = 0.5\\).</i></div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "What is the <b>maximum likelihood </b>of the <b>bernoulli </b>distribution:<div>\\(\\operatorname{Bern}(x | \\mu)=\\mu^{x}(1-\\mu)^{1-x}\\)</div>", "Back": "\\(\\mu^{\\mathrm{ML}}=\\frac{m}{N} \\quad \\text { with } \\quad m=(\\# \\text { observations of } x=1)\\)<div><br></div><div><i>Simply the number of times we find 1 divided by the total tries.</i></div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "<b>Maximum likelihood </b>of the Bernoulli distribution<div>\\(\\operatorname{Bern}(x | \\mu)=\\mu^{x}(1-\\mu)^{1-x}\\)<br></div>", "Back": "\\(\\mu^{\\mathrm{ML}}=\\frac{m}{N} \\quad \\text { with } \\quad m=(\\# \\text { observations of } x=1)\\)<div><br></div><div><i>Simply the average amount of times the random event was 1.</i></div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "<div><div>\\(\\operatorname{Bern}(x | \\mu)=\\mu^{x}(1-\\mu)^{1-x}\\)</div></div><div><br></div>What is the problem with the&nbsp;<b>Maximum likelihood&nbsp;</b>of the Bernoulli distribution<div>\\(\\mu^{\\mathrm{ML}}=\\frac{m}{N}\\)<br></div>", "Back": "It severely <b>overfits </b>for <b>small datasets</b>.<div><br></div><div><i>If a coin falls twice on 1, we'll asume all next tries will be also be 1.</i></div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "\\(p(\\mathbf{w} | \\mathcal{D})=\\frac{p(\\mathcal{D} | \\mathbf{w}) p(\\mathbf{w})}{p(\\mathcal{D})}\\)<br><div><br></div><div>When does the posterior \\(p(\\mathbf{w} | \\mathcal{D})\\) and the prior \\(p(\\mathbf{w})\\) are <b>conjugate distributions</b></div>", "Back": "When they follow the <b>same probability distribution family </b>(<i>i.e. gaussian, beta, etc.</i>)", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "\\(p(\\mathbf{w} | \\mathcal{D})=\\frac{p(\\mathcal{D} | \\mathbf{w}) p(\\mathbf{w})}{p(\\mathcal{D})}\\)<br><div><br></div><div>What is the name of the pair posterior \\(p(\\mathbf{w} | \\mathcal{D})\\) and prior \\(p(\\mathbf{w})\\) when they follow&nbsp;<b>same probability distribution family</b></div>", "Back": "<b>Conjugate distributions</b>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "\\(p(\\mathbf{w} | \\mathcal{D})=\\frac{p(\\mathcal{D} | \\mathbf{w}) p(\\mathbf{w})}{p(\\mathcal{D})}\\)<br><div><br></div><div>When the posterior \\(p(\\mathbf{w} | \\mathcal{D})\\) and the prior \\(p(\\mathbf{w})\\) are&nbsp;<b>conjugate distributions.</b></div><div><b><br></b></div><div>What is the name of the&nbsp;<b>prior to the likelihood</b>&nbsp;\\(p(\\mathcal{D} | \\mathbf{w})\\)?</div>", "Back": "<b>Conjugate prior </b>to the likelihood function", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "When does <b>bayesian </b>and <b>maximum likelihood </b>agree?", "Back": "When the dataset grows to \\(+\\infty\\).<div><br></div><div><i>Remember that the underestimation of the variance of ML is 0 when the dataset is infinite.</i></div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "Intuively, where is the <b>posterior </b>compared to the <b>likelihood </b>and <b>prior</b>?", "Back": "<div>As an <b>intermediary distribution </b>between them.</div><img src=\"paste-5813b2bb7d7ff7231af2676024b9bfd9f2da3e56.jpg\">", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "For a <b>binomial likelihood </b>function, what kind of <b>conjugate prior </b>can we chose?", "Back": "A <b>beta </b>distribution.<div><img src=\"paste-5813b2bb7d7ff7231af2676024b9bfd9f2da3e56.jpg\"><br></div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "For a <b>multinomial likelihood&nbsp;</b>function, what kind of&nbsp;<b>conjugate prior&nbsp;</b>can we chose?", "Back": "A <b>Dirichlet </b>distribution<div><img src=\"paste-4b9a845bd4923f0d7a52b0edae6087452fcddab4.jpg\"><br></div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "What is the <b>multinomial </b>distribution?", "Back": "A vector of \\(K\\) random values.<div>\\(\\mathbf{x}=(0,0,1,0,0,0)^{\\mathrm{T}}\\)<br></div><div><br></div><div>With constraints that \\(\\sum_k x_k = 1\\).</div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "To which distribution can we see the <b>multinomial</b>&nbsp;its generalization<div>\\(\\mathbf{x}=(0,0,1,0,0,0)^{\\mathrm{T}}\\)</div>", "Back": "A generalization of <b>Bernoulli </b>to \\(K\\) values.", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "Which <b>distribution</b>&nbsp;is it?<div>\\(\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(\\mathrm{x}-\\boldsymbol{\\mu})^{\\mathrm{T}} \\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right\\}\\)</div>", "Back": "<div><b>multivariate-Gaussian </b>distribution.</div><div><br></div>\\(\\mathcal{N}(\\mathrm{x} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma})=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(\\mathrm{x}-\\boldsymbol{\\mu})^{\\mathrm{T}} \\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right\\}\\)", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "Name of the following <b>distance</b>:<div>\\((\\mathbf{x}-\\mathbf{y})^{\\mathrm{T}} \\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{y})\\)</div>", "Back": "<b>Mahalanobis </b>distancd", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "Intuition of \\(\\Sigma\\) role in the <b>Mahalanobis</b> distance:<div><br><div>\\((\\mathbf{x}-\\mathbf{y})^{\\mathrm{T}} \\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{y})\\)</div></div>", "Back": "<b>Dimension with high variability will matter less</b> than dimension with low variability.<div><br></div><div>This makes sense as we are less certain of those high-variability dimensions.</div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "What is \\(\\Sigma\\) in the <b>Mahalanobis</b> distance:<div><br><div>\\((\\mathbf{x}-\\mathbf{y})^{\\mathrm{T}} \\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{y})\\)</div></div>", "Back": "The <b>covariance </b>matrix", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "How the <b>Mahalanobis</b> distance can reduce to a <b>Euclidean </b>distance?<div><br><div>\\((\\mathbf{x}-\\mathbf{y})^{\\mathrm{T}} \\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{y})\\)</div></div>", "Back": "When \\(\\Sigma = I\\), the <b>identity matrix</b>.", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "" }, { "Front": "When can be have a <b>close-form</b>&nbsp;of the <u>posterior</u>?<div>\\(p(\\theta | x)=\\frac{p(x | \\theta) p(\\theta)}{p(x)}\\)</div>", "Back": "When we have a <b>conjugate prior</b>.", "Subtitle": "Probability", "Details1": "Otherwise&nbsp;numerical integration or <u>approximation</u> are needed as we cannot have infinite data.", "Details2": "The conjugate prior and the posterior have the same <u>distribution family</u>.", "Image": "" }, { "Front": "What are the <b>free parameters </b>of a <u>Gaussian distribution</u>?", "Back": "The <b>mean </b>\\(\\mathbf{\\mu}\\) and the <b>covariance matrix </b>\\(\\mathbf{\\Sigma}\\).", "Subtitle": "Probability", "Details1": "\\(\\mathbf{\\mu}\\) has \\(D\\) parameters.", "Details2": "\\(\\mathbf{\\Sigma}\\) has \\(D(D + 1)/2\\) parameters.", "Image": "" }, { "Front": "What is the <b>computational problem </b>in a <u>Gaussian distribution</u>?<div><br><div>\\(\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma})=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right\\}\\)</div></div>", "Back": "We need to <b>inverse </b>the <u>covariance matrix</u>&nbsp;\\(\\mathbf{\\Sigma}\\).", "Subtitle": "Probability", "Details1": "\\(\\mathbf{\\Sigma}\\) has \\(D(D + 1)/2\\) parameters.", "Details2": "It can be unfeasible for very large dimension space.", "Image": "" }, { "Front": "How to <b>reduce</b>&nbsp;the number of free parameters of a <u>Gaussian distribution</u>?<br><div><br></div><div>\\(\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma})=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right\\}\\)<br></div>", "Back": "By considering <u>covariance matrices</u>&nbsp;that are <b>diagonal</b>, so that \\(\\mathbf{\\Sigma} = \\text{diag}(\\sigma_i^2)\\).", "Subtitle": "Probability", "Details1": "The covariance matrix now only have \\(D\\) parameters, with the mean \\(2D\\).", "Details2": "We can reduce furthermore the number of parameters with <u>isotropic</u>&nbsp;covariance \\(\\mathbf{\\Sigma} = \\sigma^2\\mathbf{I}\\).", "Image": "<img src=\"paste-24af954c5f8b95e259e85c28075745986400c8ad.jpg\">" }, { "Front": "Interpret intuitively a <u>covariance matrix</u>&nbsp;that is <b>diagonal</b>: \\(\\mathbf{\\Sigma} = \\text{diag}(\\sigma_i^2)\\).", "Back": "All dimensions are independents to each other \\(\\sigma_{ij} = 0\\).", "Subtitle": "Probability", "Details1": "This reduces the number of parameters from \\(D(D+1)/2\\) to \\(D\\).", "Details2": "", "Image": "" }, { "Front": "How can we have a&nbsp;<u>covariance matrix</u>&nbsp;with a <b>single parameter</b>?", "Back": "With \\(\\mathbf{\\Sigma} = \\sigma^2\\mathbf{I}\\).", "Subtitle": "Probability", "Details1": "The matrix is called <u>isotropic</u>.", "Details2": "", "Image": "<img src=\"paste-c0bf9af3550541f17ffcced7bab381f3938d56e7.jpg\">" }, { "Front": "<div>Which contour is parametrized by a <b>diagonal </b><u>covariance matrix,</u>&nbsp;and why?</div><div><br></div><img src=\"paste-f272ad1b91c37544739dc9cb0c51e9912f9af294.jpg\">", "Back": "The (b), because the elliptical contours are <u>aligned</u> to the axes as <b>dimensions are independent</b>&nbsp;to each others.", "Subtitle": "Probability", "Details1": "\\(\\mathbf{\\Sigma} = \\text{diag}(\\sigma_i^2)\\)", "Details2": "\\(\\sigma_{ij} = 0\\)", "Image": "" }, { "Front": "<div>Which contour is parametrized by an&nbsp;<b>isotropic&nbsp;</b><u>covariance matrix,</u>&nbsp;and why?</div><div><br></div><img src=\"paste-f272ad1b91c37544739dc9cb0c51e9912f9af294.jpg\">", "Back": "The (c), because the contours are <u>concentric circles</u>&nbsp;as it is parametrized by a <b>single parameter</b>.", "Subtitle": "Probability", "Details1": "\\(\\mathbf{\\Sigma} = \\sigma_i \\mathbf{I}\\)", "Details2": "", "Image": "" }, { "Front": "What is an <b>isotropic </b>covariance matrix?<div></div>", "Back": "\\(\\mathbf{\\Sigma} = \\sigma_i \\mathbf{I}\\)", "Subtitle": "Probability", "Details1": "It is parametrized by a single parameter.", "Details2": "", "Image": "<img src=\"paste-808cc659c19123c34f4662b7d324a4cf5195dda8.jpg\">" }, { "Front": "<div></div><div>In what ways are <u>gaussian distributions</u>&nbsp;<b>limited</b>?</div>", "Back": "They are <b>unimodal</b>, and thus failed to represent multimodal distributions.", "Subtitle": "Probability", "Details1": "unimodal = has a single maximum", "Details2": "Most real-world distributions are multimodal.", "Image": "<img src=\"paste-f272ad1b91c37544739dc9cb0c51e9912f9af294.jpg\">" }, { "Front": "What is the name of the <b>inverse of the covariance</b>?<div>\\(\\Lambda \\equiv \\Sigma^{-1}\\)</div>", "Back": "The <b>precision</b>.", "Subtitle": "Probability", "Details1": "Intuitively, the higher the precision, the lower the covariance.", "Details2": "", "Image": "" }, { "Front": "<b>Marginalization</b>&nbsp;of a <u>Gaussian distribution</u>?", "Back": "Taking the distribution of a single dimension \\(x_a\\).<div><br></div><div>\\(\\boldsymbol{\\mu}=\\left(\\begin{array}{c}\\boldsymbol{\\mu}_{a} \\\\ \\boldsymbol{\\mu}_{b}\\end{array}\\right)\\).</div>", "Subtitle": "Probability", "Details1": "The result is also <u>gaussian</u>.", "Details2": "", "Image": "" }, { "Front": "Family of the <b>conditionning</b> of one <u>gaussian</u>&nbsp;by another?", "Back": "It is also a <b>gaussian</b>.", "Subtitle": "Probability", "Details1": "We say that the family is <u>closed</u>&nbsp;to conditionning.", "Details2": "It also works with <u>marginalization</u>.", "Image": "" }, { "Front": "Marginal distribution of \\(\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma}) \\text { with } \\mathbf{\\Lambda} \\equiv \\mathbf{\\Sigma}^{-1}\\)?", "Back": "\\(p\\left(\\mathbf{x}_{a}\\right)=\\mathcal{N}\\left(\\mathbf{x}_{a} | \\boldsymbol{\\mu}_{a}, \\boldsymbol{\\Sigma}_{a a}\\right)\\)", "Subtitle": "Probability", "Details1": "The marginalization of a gaussian distribution is also gaussian.", "Details2": "", "Image": "<img src=\"paste-f7aab8bacdf3a2843e3b7e068f77a748c621b6b1.jpg\">" }, { "Front": "Conditional distribution of \\(\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma}) \\text { with } \\mathbf{\\Lambda} \\equiv \\mathbf{\\Sigma}^{-1}\\)?", "Back": "\\(\\begin{aligned} p\\left(\\mathbf{x}_{a} | \\mathbf{x}_{b}\\right) &amp;=\\mathcal{N}\\left(\\mathbf{x} | \\boldsymbol{\\mu}_{a | b}, \\boldsymbol{\\Lambda}_{a a}^{-1}\\right) \\\\ \\boldsymbol{\\mu}_{a | b} &amp;=\\boldsymbol{\\mu}_{a}-\\boldsymbol{\\Lambda}_{a a}^{-1} \\boldsymbol{\\Lambda}_{a b}\\left(\\mathbf{x}_{b}-\\boldsymbol{\\mu}_{b}\\right) \\end{aligned}\\)", "Subtitle": "Probability", "Details1": "The conditional of a gaussian distribution is also gaussian.", "Details2": "", "Image": "<img src=\"paste-f7aab8bacdf3a2843e3b7e068f77a748c621b6b1.jpg\">" }, { "Front": "In a <b style=\"\">sequential setting</b>, how to interpret this result?<div><br></div><div>\\(\\begin{aligned} \\boldsymbol{\\mu}_{\\mathrm{ML}}^{(N)} &amp;=\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_{n} \\\\ &amp;=\\frac{1}{N} \\mathbf{x}_{N}+\\frac{1}{N} \\sum_{n=1}^{N-1} \\mathbf{x}_{n} \\\\ &amp;=\\frac{1}{N} \\mathbf{x}_{N}+\\frac{N-1}{N} \\boldsymbol{\\mu}_{\\mathrm{ML}}^{(N-1)} \\\\ &amp;=\\boldsymbol{\\mu}_{\\mathrm{ML}}^{(N-1)}+\\frac{1}{N}\\left(\\mathbf{x}_{N}-\\boldsymbol{\\mu}_{\\mathrm{ML}}^{(N-1)}\\right) \\end{aligned}\\)</div>", "Back": "We move the old estimate by a fraction \\(1/N\\) of the <u>error signal</u>&nbsp;\\(\\left(\\mathbf{x}_{N}-\\boldsymbol{\\mu}_{\\mathrm{ML}}^{(N-1)}\\right)\\).", "Subtitle": "", "Details1": "Contribution from successive data points get smaller.", "Details2": "At some points, the estimate converge.", "Image": "" }, { "Front": "How does the <b>eigenvectors</b>&nbsp;of the <u>covariance</u>&nbsp;\\(\\Sigma\\) affect this Gaussian distribution?<div><img src=\"paste-874a64df396aecbf36473c40a4e4c6d328189523.jpg\"><br></div>", "Back": "It affect the <i>orientation</i>&nbsp;of the elliptical surface.", "Subtitle": "Probability", "Details1": "With \\(u_1\\) and \\(u_2\\) the <u>eigenvectors</u>.", "Details2": "Note that \\(\\lambda_1\\) and \\(\\lambda_2\\) are the&nbsp;<u>eigenvalues</u>.", "Image": "<img src=\"paste-3e0c82de4f5f5febaf5f430de0989421682190f7.jpg\">" }, { "Front": "How does the <b>eigenvalues</b>&nbsp;of the <u>covariance</u>&nbsp;\\(\\Sigma\\) affect this Gaussian distribution?<div><img src=\"paste-874a64df396aecbf36473c40a4e4c6d328189523.jpg\"><br></div>", "Back": "It affect the <i>spread</i>&nbsp;of the elliptical surface.", "Subtitle": "Probability", "Details1": "With \\(\\lambda_1\\) and \\(\\lambda_2\\)&nbsp;the&nbsp;<u>eigenvalues</u>.", "Details2": "Note that \\(u_1\\) and \\(u_2\\)&nbsp;are the&nbsp;<u>eigenvectors</u>.", "Image": "<img src=\"paste-3e0c82de4f5f5febaf5f430de0989421682190f7.jpg\">" }, { "Front": "<div>With \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\) two <u>Gaussians</u>, describe the mean of the <b>conditionnal gaussian&nbsp;</b>\\(p(\\mathbf{y} | \\mathbf{x})\\).</div>", "Back": "<div>It is a <b>linear transformation </b>of \\(\\mathbf{x}\\)'s mean:</div><div><br></div>\\(p(\\mathbf{y} | \\mathbf{x})=N\\left(\\mathbf{y}, \\mathbf{A} \\mathbf{x}+\\mathbf{b}, L^{-1}\\right)\\)", "Subtitle": "Probability", "Details1": "We call this a <u>linear gaussian model</u>.", "Details2": "", "Image": "" }, { "Front": "Given <u>gaussian</u> data, what is the distribution of the <b>conjugate prior </b>\\(p(\\mu)\\)?", "Back": "A <b>gaussian </b>distribution: \\(p(\\mu)=\\mathcal{N}\\left(\\mu | \\mu_{0}, \\sigma_{0}^{2}\\right)\\)", "Subtitle": "Probability", "Details1": "The posterior \\(p(\\mu | \\mathbf{X}) \\propto p(\\mathbf{X} | \\mu) p(\\mu)\\) is therefore also&nbsp;<u>gaussian</u>.", "Details2": "In this case, the parameter \\(\\sigma^2\\) is known while \\(\\mu\\) unknown.", "Image": "" }, { "Front": "Given <u>gaussian</u> data, what is the distribution of the <b>conjugate prior </b>\\(p(\\frac{1}{\\sigma^2}) = p(\\lambda)\\)?", "Back": "A <b>gamma&nbsp;</b>distribution: \\(\\operatorname{Gam}(\\lambda | a, b)=\\frac{1}{\\Gamma(a)} b^{a} \\lambda^{a-1} \\exp (-b \\lambda)\\)", "Subtitle": "Probability", "Details1": "The posterior \\(p(\\mu | \\mathbf{X}) \\propto p(\\mathbf{X} | \\mu) p(\\mu)\\) is therefore also <u>gaussian</u>.", "Details2": "In this case, the parameter \\(\\mu\\) is known while \\(\\sigma^2\\) unknown.", "Image": "<img src=\"paste-8b2a48bb98d3a5e59e3fdbf6b339d184dc1706b2.jpg\">" }, { "Front": "What is this function name \\(\\text{?}(n)=(n-1)!\\)", "Back": "The <b>Gamma </b>function \\(\\Gamma(n)=(n-1)!\\).", "Subtitle": "Probability", "Details1": "It's the <u>normalizing factor</u>&nbsp;in the <u>Gamma</u>&nbsp;distribution \\(\\operatorname{Gam}(\\lambda | a, b)=\\frac{1}{\\Gamma(a)} b^{a} \\lambda^{a-1} \\exp (-b \\lambda)\\).", "Details2": "The Gamma&nbsp;distribution&nbsp;as prior for&nbsp;<u>gaussian</u>&nbsp;data when \\(\\sigma^2\\) is unknown.", "Image": "" }, { "Front": "Given&nbsp;<u>gaussian</u>&nbsp;data, what is the distribution of the&nbsp;<b>conjugate prior&nbsp;</b>\\(p(\\mu, \\frac{1}{\\sigma^2}) = p(\\mu, \\lambda)\\)?", "Back": "The <b>normal gamma </b>distribution: \\(N\\left(\\mu | \\mu_{0}, \\lambda_{0}^{-1}\\right) \\operatorname{Gam}(\\lambda | a, b)\\).", "Subtitle": "Probability", "Details1": "The prior for \\(\\mu\\) is <u>gaussian</u>&nbsp;and for \\(sigma^2\\) <u>gamma</u>.", "Details2": "If the gaussian is <u>multivariate</u>, the prior is the <u>Gaussian-Wishart</u>&nbsp;distribution.", "Image": "<img src=\"paste-377ae46e68153f17978d21ab1d745580d2158d16.jpg\">" }, { "Front": "Relation of the <b>t-Student </b>distribution to the <u>Gaussian</u>&nbsp;distribution?", "Back": "The student&nbsp;distribution is an <b>infinite sum of Gaussian</b>, with same mean but <u>different precisions</u>:<div><br></div><div><div>\\(p(x | \\mu, a, b)=\\int_{0}^{\\infty} N\\left(x | \\mu, \\tau^{-1}\\right) \\operatorname{Gam}(\\tau | a, b) d \\tau\\)</div></div>", "Subtitle": "Probability", "Details1": "", "Details2": "", "Image": "<img src=\"paste-f82e7a4b94425074e64d4fa4024c5f9fc3988d66.jpg\">" }, { "Front": "Why does the <b><font color=\"#fc0107\">Student</font></b>&nbsp;distribution&nbsp;is less sensible to <u>outliers</u>&nbsp;than the <font color=\"#21ff06\">Gaussian</font> distribution?<div><img src=\"paste-b82309ec14384305249bcc3f2606d0349b377861.jpg\"><br></div>", "Back": "Because the Student distribution has a <b>longer tails</b>.", "Subtitle": "Probability", "Details1": "Outliers are catched by this long tails without alterating the main mode.", "Details2": "Regression can be more robust with a Student distribution.", "Image": "<img src=\"paste-32c7f57c209de91c9a0dea7255e15b0c29ea0b96.jpg\">" }];

    visualize({ randomOrder: false, title: "Introduction to Deep Learning" });
</script>

</html>
